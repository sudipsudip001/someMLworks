{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01089a0f-7b1b-450c-b985-8d7b608603a6",
   "metadata": {},
   "source": [
    "9:04 am 10th june, 2024:\n",
    "In this notebook I'll be train a RNN on the word 'dogs' for quite some time such that when I supply it with the word 'dog' it should be able to predict the word 's'. I'm not quite sure if to train it on multiple words like 'dogs dune drunk dust damp dance ...' or just 'dogs dogs dogs dogs ...'. I think things will be clear as I move along. I've had my endeavors for past 3 days to understand the min-char-rnn.py by the OG himself, Mr. Karpathy. I hope to implement my own today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3119112-c2c4-497a-b102-26f429a53cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2b0d35-6f73-422a-8b67-778390a5e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first try to only iterate with the word dogs\n",
    "data = 'dogs'\n",
    "chars = list(dict.fromkeys(data))\n",
    "size = len(data)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab6d85-5121-45ad-b23f-f72cf489d925",
   "metadata": {},
   "source": [
    "I'll just keep about 5 hidden neurons 'arbitrarily' for now. I'm a bit confuesed about the seq_length's size. In Mr. Karpathy's code it's value was 25.\n",
    "I'll try with 3 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7198bc10-9eb0-49a8-aff3-e60b98d3021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 6\n",
    "seq_length = 3\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ec81da-1d5a-4f10-861d-fd18401a5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh = np.random.randn(hidden_size, size)*0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01\n",
    "Why = np.random.randn(size, hidden_size)*0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01a5dee-8aa0-4a07-a86d-1a8ec7c27168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((size,1))\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "    ys[t] = np.dot(Why, hs[t]) + by\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "    loss += -np.log(ps[t][targets[t],0])\n",
    "\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam)\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ec55b2-20a9-4671-a2ec-782e190b3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  x = np.zeros((size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(size), p=p.ravel())\n",
    "    x = np.zeros((size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f57553d-13da-4021-8956-17819e319b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
    "smooth_loss = -np.log(1.0/size)*seq_length\n",
    "while smooth_loss > 0.001:\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1))\n",
    "    p = 0\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  sample_ix = sample(hprev, inputs[0], 3)\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  # print('iter %d, loss: %f' % (n, smooth_loss))\n",
    "  \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "  p += seq_length\n",
    "  n += 1\n",
    "    \n",
    "text = 'd'\n",
    "for i in range(3):\n",
    "    ix = sample_ix[i]\n",
    "    text += ix_to_char[ix]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03507166-b427-4ff4-b43b-5b8811e99abe",
   "metadata": {},
   "source": [
    "10:31am It's done. Did a while ago but writing it just now. Most of the works are simple copies from Mr. Karpathy's code. I just did a little \"tinkering\" with it to get what I wanted. So basically in the program what happens is we supply the training word 'dogs' to the model. Throughout the whole iterations, what is does is simply learn what word is the most likely one to come after a particular word. i.e. in the word 'dogs' if we supply the program with 'd' in the training phase then the program calculates the likelihood of getting 'o' or we train it such as way that it should be getting 'o'. If it doesn't there's error that's calculated which will be used to thus correct the corresponding weights and biases such that the output is always 'o' after we've inputted 'd'. It iterates in this way for as long as the error calculated isn't less than 0.001 in above case.\n",
    "\n",
    "Finally when the error is below a particular level, we then print the letters following 'd' which will have been stored in the sample_ix variable. Thus, in this way we're able to predict 'dogs' from the word 'd' only by training the model in that way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
